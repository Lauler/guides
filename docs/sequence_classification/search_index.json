[["index.html", "Document Classification with Transformers Welcome", " Document Classification with Transformers Faton Rekathati, KBLab 2022-03-30 Welcome KBLab is a creator of open source AI models and tools. We publish and share models that we have trained under the organizations KB and KBLab on huggingface.co. While the pre-training of these models require substantial compute resources and data, using an already pre-trained model and customizing it for a specific task is considerably less resource intensive. Most tasks will not require more than a regular consumer workstation to produce impressive results. At KBLab we would like for more people to be able to make practical use our models – both in research and in an applied setting. Thus in an effort to make our models more accessible, we are publishing a series of guides demonstrating their usage by way of example on real world datasets. In this guide we cover document classification: the task of assigning a document to either one of two categories (binary classification) or one of several available categories (multinomial classification). Throughout this guide you will encounter colorful boxes providing advice and tips for the reader. Below, we briefly explain the purpose and function of each box. This box contains notes and extras with useful commentary on the concepts we cover. It is not strictly necessary to read the information in the green boxes to follow along with the examples, but it might help with your understanding. We use the orange boxes to describe and link to useful external resources where you can learn more. They are also used to present alternative code solutions. The red boxes alert the reader about common mistakes and pitfalls, in the hopes that the reader can avoid having to suffer through some of the same glorious debugging sessions the authors had to endure. "],["intro.html", "Chapter 1 Document Classifiation 1.1 Examples of classification tasks 1.2 Expected data 1.3 Loading the model 1.4 Loading the tokenizer", " Chapter 1 Document Classifiation In text classification we are looking to assign documents to predefined categories. We call our input to the model a text sequence, where sequence can be a text of arbitrary length. In many NLP contexts you will encounter the term sentence being used interchangeably as a synonym for sequence. This can be rather confusing for beginners, but has become established since some early transformer papers took the liberty of loosening the definition of the term sentence. TODO: Create similar image as above (source d2l.ai) 1.1 Examples of classification tasks Examples of text classification include any task where we wish to assign documents to discrete categories. We list some examples usages: Politics: Determine the political lean of a text. Sentiment analysis: Does a text express positive or negative sentiment in contexts where your product/organization is mentioned? Toxicity: Can we classify toxic or threatening comments posted on our website and automatically flag them for review? E-mails: Categorizing incoming e-mails and automatically suggesting the correct department in your organization they should be forwarded to. 1.2 Expected data 1.3 Loading the model In our examples we will be using BertForSequenceClassification to load KB’s Swedish BERT model for classification tasks. This will automatically attach a dense classifier head on top of the pre-trained BERT model’s attention layers. import torch from transformers import BertForSequenceClassification device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) model = BertForSequenceClassification.from_pretrained(&quot;KB/bert-base-swedish-cased&quot;, num_labels=2) model.to(device) model.train() Naming the layer “pooler” is slightly misleading. A pooling layer generally implies calculating a summary statistic across some feature dimension. For example the element wise averaging of multiple stacked token embedding vectors. However, in the transformers library The BertPooler only extracts the first token tensor (corresponding to [CLS]). Thus they are not pooling in the traditional expected sense of averaging across several vectors. They apply a dense layer that “projects” the vector to an output of identical size. Indeed, in the documentation for BertPooler, they themselves put the word pool in quotation marks: # We &quot;pool&quot; the model by simply taking the hidden state corresponding # to the first token. 1.4 Loading the tokenizer Figure 1.1: Our transformer model expects integer inputs. Therefore we tokenize our input text. Each token maps to an integer id in our vocabulary consisting of \\(\\mathbf{50325}\\) tokens. With the help of these integer ids, the model can select the correct input token embedding from its ‘vocabulary of embeddings’ (\\(\\mathbf{50325 \\times 768}\\)). "],["sentiment-analysis-on-scandisent.html", "Chapter 2 Sentiment Analysis on ScandiSent 2.1 Download the data 2.2 Import libraries and set device 2.3 Read data 2.4 Create Dataset and DataLoader 2.5 Training loop", " Chapter 2 Sentiment Analysis on ScandiSent 2.1 Download the data 2.2 Import libraries and set device import pandas as pd import torch from tqdm import tqdm from torch.utils.data import Dataset from transformers import BertForSequenceClassification, AutoTokenizer, AdamW device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) print(device) ## cuda 2.3 Read data We split the data the same way Isbister, Carlsson, and Sahlgren (2021) did. df = pd.read_csv(&quot;ScandiSent/sv.csv&quot;) df_train = df[:7500] # First 7500 train set df_valid = df[7500:] # Last 2500 evaluation Let us take a look at the ScandiSent data again. 2.4 Create Dataset and DataLoader class SentimentDataset(Dataset): def __init__(self, df): self.df = df self.tokenizer = AutoTokenizer.from_pretrained(&quot;KB/bert-base-swedish-cased&quot;) def __len__(self): return len(self.df) def __getitem__(self, index): df_row = self.df.iloc[index] label = df_row[&quot;label&quot;] text = df_row[&quot;text&quot;] tokenized_text = self.tokenizer( text, padding=True, truncation=True, max_length=512, return_tensors=&quot;pt&quot;, ) label = torch.tensor(label) tokenized_text[&quot;label&quot;] = label return tokenized_text 2.4.1 DataLoader train_dataset = SentimentDataset(df=df_train) valid_dataset = SentimentDataset(df=df_valid) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=16, collate_fn=custom_collate_fn, shuffle=True, num_workers=4 ) valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=16, collate_fn=custom_collate_fn, shuffle=False, num_workers=4 ) 2.4.2 Data collator with padding def custom_collate_fn(data): tokens = [sample[&quot;input_ids&quot;][0] for sample in data] attention_masks = [sample[&quot;attention_mask&quot;][0] for sample in data] labels = [sample[&quot;label&quot;] for sample in data] attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True) padded_tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True) labels = torch.stack(labels) # List of B 1-length vectors to single vector of dimension B batch = {&quot;input_ids&quot;: padded_tokens, &quot;attention_mask&quot;: attention_masks, &quot;labels&quot;: labels} return batch 2.5 Training loop Everything you always wanted to know about padding and truncation. References "],["methods.html", "Chapter 3 Classify Parliamentary Motions 3.1 Download the data 3.2 Import libraries and set device 3.3 Read data 3.4 Create Dataset and DataLoader 3.5 Training loop", " Chapter 3 Classify Parliamentary Motions 3.1 Download the data 3.2 Import libraries and set device import pandas as pd import torch from sklearn import metrics from tqdm import tqdm from torch.utils.data import Dataset from transformers import BertForSequenceClassification, AutoTokenizer, AdamW device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) 3.3 Read data df = pd.read_feather(&quot;motioner_2018_2021.feather&quot;) df = df[df[&quot;single_party_authors&quot;] == True] # df = df[df[&quot;subtyp&quot;] == &quot;Enskild motion&quot;] label_mapping = { 0: &quot;V&quot;, 1: &quot;S&quot;, 2: &quot;MP&quot;, 3: &quot;C&quot;, 4: &quot;L&quot;, 5: &quot;M&quot;, 6: &quot;KD&quot;, 7: &quot;SD&quot;, 8: &quot;independent&quot;, } label_mapping = {v: k for k, v in label_mapping.items()} df[&quot;label&quot;] = df[&quot;party&quot;].map(label_mapping) df = df.reset_index(drop=True) 3.3.1 Split into train and validation df_train = df.sample(frac=0.85, random_state=5) df_valid = df.drop(df_train.index) 3.4 Create Dataset and DataLoader class MotionerDataset(Dataset): def __init__(self, df): self.df = df self.tokenizer = AutoTokenizer.from_pretrained(&quot;KB/bert-base-swedish-cased&quot;) def __len__(self): return len(self.df) def __getitem__(self, index): df_row = self.df.iloc[index] label = df_row[&quot;label&quot;] text = df_row[&quot;text&quot;] tokenized_text = self.tokenizer( text, padding=True, truncation=True, max_length=512, return_tensors=&quot;pt&quot;, ) label = torch.tensor(label) tokenized_text[&quot;label&quot;] = label return tokenized_text 3.4.1 DataLoader train_dataset = MotionerDataset(df=df_train) valid_dataset = MotionerDataset(df=df_valid) train_loader = torch.utils.data.DataLoader( train_dataset, batch_size=16, collate_fn=custom_collate_fn, shuffle=True, num_workers=4 ) valid_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=16, collate_fn=custom_collate_fn, shuffle=False, num_workers=4 ) 3.4.2 Data collator with padding def custom_collate_fn(data): tokens = [sample[&quot;input_ids&quot;][0] for sample in data] attention_masks = [sample[&quot;attention_mask&quot;][0] for sample in data] labels = [sample[&quot;label&quot;] for sample in data] attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True) padded_tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True) labels = torch.stack(labels) # List of B 1-length vectors to single vector of dimension B batch = {&quot;input_ids&quot;: padded_tokens, &quot;attention_mask&quot;: attention_masks, &quot;labels&quot;: labels} return batch 3.5 Training loop log_list = [] for epoch in range(4): print(f&quot;epoch: {epoch + 1} started&quot;) running_loss = 0 for i, batch in enumerate(tqdm(train_loader)): optim.zero_grad() # [batch_size, 1, seq_len] -&gt; [batch_size, seq_len] input_ids = batch[&quot;input_ids&quot;].to(device) attention_mask = batch[&quot;attention_mask&quot;].to(device) labels = batch[&quot;labels&quot;].to(device) outputs = model(input_ids, attention_mask=attention_mask) loss = loss_fn(outputs[&quot;logits&quot;], labels) running_loss += loss.item() if i % 50 == 49: print(f&quot;iter: {i+1}, loss: {running_loss/50:.8f}, lr: {scheduler.get_last_lr()}&quot;) log_list.append({&quot;iter&quot;: i + 1, &quot;loss&quot;: running_loss / 50}) running_loss = 0 loss.backward() optim.step() scheduler.step() "],["add-extra-features-to-classifier.html", "Chapter 4 Add extra features to classifier", " Chapter 4 Add extra features to classifier "],["upload-to-huggingface.html", "Chapter 5 Upload to Huggingface", " Chapter 5 Upload to Huggingface "],["deploy-model-with-torchserve.html", "Chapter 6 Deploy Model with torchserve", " Chapter 6 Deploy Model with torchserve torch-model-archiver \\ --model-name text_model \\ --version 1.0 \\ --serialized-file motions.pt \\ --handler text_classifier mkdir model_store mv text_model.mar model_store/ "],["data-preprocessing-code.html", "A Data preprocessing code", " A Data preprocessing code import os import json import pandas as pd import multiprocessing as mp from bs4 import BeautifulSoup, NavigableString def read_motion_json(filename, folder=&quot;data_json&quot;): with open(os.path.join(folder, filename), &quot;r&quot;) as f: motion_json = json.load(f) return motion_json def author_party_count(motion): authors = motion[&quot;dokumentstatus&quot;][&quot;dokintressent&quot;][&quot;intressent&quot;] party_count = { &quot;V&quot;: 0, &quot;S&quot;: 0, &quot;MP&quot;: 0, &quot;C&quot;: 0, &quot;L&quot;: 0, &quot;KD&quot;: 0, &quot;M&quot;: 0, &quot;SD&quot;: 0, &quot;-&quot;: 0, } if isinstance(authors, dict): authors = [authors] for author in authors: party_count[author[&quot;partibet&quot;]] += 1 return party_count def parse_html(motion): motion_html = motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;html&quot;] soup = BeautifulSoup(motion_html, &quot;html.parser&quot;) if motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;status&quot;] == &quot;Utgången&quot;: return None # Remove signatures at bottom of bill that contain party affiliations for signature in soup.find_all( &quot;p&quot;, attrs={&quot;style&quot;: &quot;-aw-sdt-tag:CC_Underskrifter; -aw-sdt-title:CC_Underskrifter&quot;} ): signature.extract() for table in soup.find_all(&quot;table&quot;): table.extract() # Add periods to headers, so we can segment sentences properly later. for title in soup.find_all([&quot;h1&quot;, &quot;h2&quot;, &quot;h3&quot;]): for span in title.find_all(&quot;span&quot;): span.insert(len(span.get_text()), NavigableString(&quot;.&quot;)) # We only want text after motion has started if soup.find(&quot;a&quot;, attrs={&quot;name&quot;: &quot;MotionsStart&quot;}) is not None: motion_start = soup.find(&quot;a&quot;, attrs={&quot;name&quot;: &quot;MotionsStart&quot;}) else: motion_start = soup.find(&quot;h1&quot;) soup = motion_start.find_all_next() soup = BeautifulSoup(&quot;&quot;.join([str(tag) for tag in soup]), &quot;html.parser&quot;) # Remove header tags because header text is duplicated in span tags. for title in soup.find_all([&quot;h1&quot;, &quot;h2&quot;, &quot;h3&quot;]): title.extract() motion_text = &quot;&quot;.join(tag.get_text() for tag in soup) motion_text = &quot; &quot;.join(motion_text.split()) # Remove newlines, excessive whitespace party_count = author_party_count(motion) motion_fields = { &quot;hangar_id&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;hangar_id&quot;], &quot;dok_id&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;dok_id&quot;], &quot;organ&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;organ&quot;], &quot;subtyp&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;subtyp&quot;], &quot;titel&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;titel&quot;], &quot;subtitel&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;subtitel&quot;], &quot;dokument_url_html&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;dokument_url_html&quot;], &quot;datum&quot;: motion[&quot;dokumentstatus&quot;][&quot;dokument&quot;][&quot;datum&quot;], &quot;text&quot;: motion_text, &quot;party_count&quot;: [party_count], } return motion_fields def get_motion_text(filename, folder=&quot;data_json&quot;): motion_json = read_motion_json(filename, folder) motion_text = parse_html(motion_json) return motion_text motioner_list = os.listdir(&quot;data_json&quot;) pool = mp.Pool() res = pool.map(get_motion_text, motioner_list) pool.close() res = [motion for motion in res if motion is not None] df = pd.json_normalize(res) df[&quot;datum&quot;] = pd.to_datetime(df[&quot;datum&quot;]) df[&quot;hangar_id&quot;] = pd.to_numeric(df[&quot;hangar_id&quot;]) df_authors = pd.DataFrame(df[&quot;party_count&quot;].explode().tolist()).add_prefix(&quot;authors_&quot;) df_authors[&quot;nr_authors&quot;] = df_authors.sum(axis=1) df_authors = df_authors.rename(columns={&quot;authors_-&quot;: &quot;authors_independent&quot;}) df_authors[&quot;party&quot;] = (df_authors.iloc[:, 0:9] &gt;= 1).apply( lambda col: &quot;,&quot;.join(col.index[col].str.slice(8)), axis=1 ) df = pd.concat([df.drop(&quot;party_count&quot;, axis=1), df_authors], axis=1) df[&quot;single_party_authors&quot;] = ( df[ [ &quot;authors_V&quot;, &quot;authors_S&quot;, &quot;authors_MP&quot;, &quot;authors_C&quot;, &quot;authors_L&quot;, &quot;authors_KD&quot;, &quot;authors_M&quot;, &quot;authors_SD&quot;, &quot;authors_independent&quot;, ] ] &gt;= 1 ).sum(axis=&quot;columns&quot;) df[&quot;single_party_authors&quot;] = df[&quot;single_party_authors&quot;] &lt;= 1 df[&quot;text&quot;] = df[&quot;text&quot;].str.replace(&quot;^\\.&quot;, &quot;&quot;) df[&quot;text&quot;] = df[&quot;text&quot;].str.replace(&quot;^Motivering.&quot;, &quot;&quot;).str.strip() df[&quot;text&quot;] = df[&quot;text&quot;].str.replace(&quot;^Bakgrund.&quot;, &quot;&quot;).str.strip() df[&quot;text&quot;] = df[&quot;text&quot;].str.replace(&quot;^Inledning&quot;, &quot;&quot;).str.strip() # https://stackoverflow.com/questions/51976328/best-way-to-remove-xad-in-python df[&quot;text&quot;] = df[&quot;text&quot;].str.replace(&quot;\\xad&quot;, &quot;&quot;) df.to_feather(&quot;motioner_2018_2021.feather&quot;) "],["model-training-code.html", "B Model Training Code", " B Model Training Code "],["references.html", "References", " References "]]
